{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jinzhao-Yu/BioStat615/blob/main/Jinzhao_Yu_BIOSTAT615_Midterm_Project_Fall_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiAtZF-dHHBz"
      },
      "source": [
        "# Jinzhao Yu - BIOSTAT615 Midterm Project - Fall 2022"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29hcNSQSXKhU"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This midterm project aims to implement methods to <u>***classify*** </u> and <u>***cluster***</u> widely known hand-written digit database, called MNIST database. \n",
        "\n",
        "This document describes how you can read MNIST dataset in R, and how you can implement your own classification and clustering tasks. \n",
        "\n",
        "To complete your midterm project, you will need to take the following steps:\n",
        "1. Copy this document for your own version for editing\n",
        "2. Rename the document title to start with your own name: for example \"John Doe - BIOSTAT615 Midterm Project - Fall 2022\".\n",
        "3. Read this document carefully to understand what are required and recommended tasks\n",
        "4. Complete the required (and optional) tasks\n",
        "5. Write your own report in your copy of notebook directly\n",
        "6. Share your copy of the notebook with instructors (hmkang@umich.edu , cywww@umich.edu) as viewers (***make sure that you do not share it with anyone else***), and submit the URL to the notebook in Canvas.\n",
        "\n",
        "You should be able to use Markdown language to write your own report. See [Markdown Guide](https://colab.research.google.com/notebooks/markdown_guide.ipynb) to learn how to modify your copy of this report using Markdown. You may also want to read [LaTeX Tutorial](https://en.wikibooks.org/wiki/LaTeX/Mathematics) to learn how to write mathematical equations in your report. Alternatively, you may import equations as external images if you prefer.\n",
        "\n",
        "***The parts you need to complete are underlined. <u>Please make sure that you filled in all underlined parts</u>***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNMPLzUBXG8w"
      },
      "source": [
        "## Background\n",
        "\n",
        "This section introduces what the MNIST dataset looks like, and explains how you can use them for your project.\n",
        "\n",
        "### Preambles\n",
        "\n",
        "First, let's try to load some R packages we will use for some of the example exercises."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW9cajZgxcPr",
        "outputId": "43b27792-a7d4-4a3f-84cf-df25c60da78c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if (!require(\"tictoc\")) {\n",
        "    install.packages(\"tictoc\")\n",
        "}\n",
        "library(ggplot2)\n",
        "library(dplyr)\n",
        "library(tictoc)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading required package: tictoc\n",
            "\n",
            "Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n",
            "“there is no package called ‘tictoc’”\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "\n",
            "Attaching package: ‘dplyr’\n",
            "\n",
            "\n",
            "The following objects are masked from ‘package:stats’:\n",
            "\n",
            "    filter, lag\n",
            "\n",
            "\n",
            "The following objects are masked from ‘package:base’:\n",
            "\n",
            "    intersect, setdiff, setequal, union\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3tym2Lcq5v-"
      },
      "source": [
        "### Loading MNIST dataset\n",
        "\n",
        "[MNIST](http://yann.lecun.com/exdb/mnist/) is a widely recognized databases of handwritten digits, containing thousands of labeled images in a standardized form. This data has been one of the gold-standard dataset to evaluate various machine learning methods. In this project, we will NOT require implement any machine learning or deep learning methods, but will use these data as an easy-to-visualize high-dimensional data. \n",
        "\n",
        "The code fragment below will load train (n=60,000) and test (n=10,000) images and labels (0 to 9), and visualize the images to improve your understanding. First, we will download these data from the web "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vEZcgrKcRLX"
      },
      "source": [
        "## DO NOT MODIFY THIS CELL\n",
        "## set up the location of MNIST files\n",
        "baseUrl = \"http://yann.lecun.com/exdb/mnist/\"\n",
        "trainXf = \"train-images-idx3-ubyte.gz\"  ## train images\n",
        "trainYf = \"train-labels-idx1-ubyte.gz\"  ## train labels\n",
        "testXf = \"t10k-images-idx3-ubyte.gz\"    ## test images\n",
        "testYf = \"t10k-labels-idx1-ubyte.gz\"    ## test labels"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpYRV0FHdPkm"
      },
      "source": [
        "## DO NOT MODIFY THIS CELL\n",
        "## Download MNIST data\n",
        "filenames = c(trainXf, trainYf, testXf, testYf)\n",
        "for(fn in filenames) { \n",
        "  if ( !file.exists(fn) ) { ## skip download if already exists\n",
        "    system(paste0(\"wget \",baseUrl, fn),intern=TRUE)\n",
        "  }\n",
        "}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pl_BpFBrfWvy",
        "outputId": "53186267-f477-4a8f-a43b-45a207f7ff9c"
      },
      "source": [
        "## DO NOT MODIFY THIS CELL\n",
        "## verify the downloaded files\n",
        "print(system(\"ls -l\",intern=TRUE))\n",
        "## Below is what you are expected to see when run this cell"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"total 11340\"                                                           \n",
            "[2] \"drwxr-xr-x 1 root root    4096 Nov 18 14:34 sample_data\"               \n",
            "[3] \"-rw-r--r-- 1 root root 1648877 Jul 21  2000 t10k-images-idx3-ubyte.gz\" \n",
            "[4] \"-rw-r--r-- 1 root root    4542 Jul 21  2000 t10k-labels-idx1-ubyte.gz\" \n",
            "[5] \"-rw-r--r-- 1 root root 9912422 Jul 21  2000 train-images-idx3-ubyte.gz\"\n",
            "[6] \"-rw-r--r-- 1 root root   28881 Jul 21  2000 train-labels-idx1-ubyte.gz\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXRucF9Pfq0U"
      },
      "source": [
        "The download data is a binary file, and it follows a specific format described in the [original web site](http://yann.lecun.com/exdb/mnist/). To help you get started with this data, an example function to load this data is provided below. You do not have to understand how this function works. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umjX2RpCh2Ke"
      },
      "source": [
        "## DO NOT MODIFY THIS CELL\n",
        "#' readMNIST() - Function to read MNIST images/labels\n",
        "#' @param filename - Path to the local binary file\n",
        "#' @return 1-d (label) or 3-d (images) arrays of MNIST data\n",
        "readMNIST = function(filename) {\n",
        "  con = gzfile(filename,\"rb\")\n",
        "  hdr = readBin(con, integer(), n=2, endian=\"big\") ## read one byte to figure out file type\n",
        "  if ( hdr[1] == 2051 ) {         ## 2051 is a magic number for image files\n",
        "    nxy = readBin(con, integer(), n=2, endian=\"big\")  \n",
        "    return ( array( readBin(con, integer(), signed=FALSE, size=1, n=hdr[2]*nxy[1]*nxy[2], endian=\"big\"), dim=c(nxy[1],nxy[2],hdr[2]) ) )\n",
        "  } else if ( hdr[1] == 2049 ) {  ## 2049 is a magic number for label files\n",
        "    return ( readBin(con, integer(), signed=FALSE, size=1, n=hdr[2], endian=\"big\") )\n",
        "  } else {\n",
        "    stop(paste0(\"Cannot recognize the magic number \", hdr[1]))\n",
        "  }\n",
        "}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6XpADxxo_jF",
        "outputId": "51832eac-61c9-4d0e-f27c-e5dc1ae559d1"
      },
      "source": [
        "## DO NOT MODIFY THIS CELL\n",
        "train.X = readMNIST(trainXf)  ## read training images\n",
        "train.Y = readMNIST(trainYf)  ## read training labels\n",
        "print(dim(train.X))           ## print the dimension of training images (3-d array)\n",
        "print(length(train.Y))        ## print the dimension of training labels"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]    28    28 60000\n",
            "[1] 60000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coo2CQ7Fktj7",
        "outputId": "0a31924f-00af-4f5c-ad53-c91718ca28f1"
      },
      "source": [
        "## DO NOT MODIFY THIS CELL\n",
        "test.X = readMNIST(testXf)  ## read test images\n",
        "test.Y = readMNIST(testYf)  ## read test labels\n",
        "print(dim(test.X))          ## print the dimension of test images\n",
        "print(length(test.Y))       ## print the dimension of test labels"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]    28    28 10000\n",
            "[1] 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "z77ZYG4mlUfi",
        "outputId": "ad86ca96-3380-441c-80c0-fef17d71f204"
      },
      "source": [
        "## DO NOT MODIFY THIS CELL\n",
        "## quick sanity checking\n",
        "print(train.X[10:15,20:15,3]) ## print some part of the large 3-d array\n",
        "## draw first 25 images: note that the images are mirrored in columns\n",
        "par(mfrow=c(5,5))\n",
        "par(mar=c(0,0,0,0))\n",
        "for(i in 1:25) { image(train.X[,28:1,i],col=gray((0:255)/255)) }"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     [,1] [,2] [,3] [,4] [,5] [,6]\n",
            "[1,]    0    0    0  177  253   47\n",
            "[2,]    0    0    0   98  254   49\n",
            "[3,]    0    0    0   56  250  116\n",
            "[4,]    0    0    0    0  240  144\n",
            "[5,]    0    0    0    0  198  150\n",
            "[6,]    0    0    0    0  143  241\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "plot without title"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAADAFBMVEUAAAABAQECAgIDAwME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\nAAAAgEn1/wHOAHCVVWpYLAAAAABJRU5ErkJggg=="
          },
          "metadata": {
            "image/png": {
              "width": 420,
              "height": 420
            }
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "KoycYw1yoHj4",
        "outputId": "4e383970-b67b-4eda-9f44-70789fd0cbea"
      },
      "source": [
        "## DO NOT MODIFY THIS CELL\n",
        "summary(train.X) ## show the summary of pixel-wise intensities\n",
        "table(train.Y)   ## show the distribution of each digit"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
              "   0.00    0.00    0.00   33.32    0.00  255.00 "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train.Y\n",
              "   0    1    2    3    4    5    6    7    8    9 \n",
              "5923 6742 5958 6131 5842 5421 5918 6265 5851 5949 "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qs1gVWQ0qTp"
      },
      "source": [
        "## Task 1 : Classification of MNIST data\n",
        "\n",
        "The first task is to implement an algorithm that performs classification of MNIST images. This task consists of two parts - (1) binary classification and (2) multi-class classification.\n",
        "\n",
        "### Task 1a : Binary Classification of MNIST data\n",
        "\n",
        "The binary classfication of MNIST data is outlined below. \n",
        "\n",
        "1. Subset images/labels that contains only two labels (you may select a pair of digits).\n",
        "2. You are required to implement logistic Ridge regression by using the L-BFGS-B algorithm implemented in `optim()` function.\n",
        "  * You need to comment your code well and write a brief documentation to explain how you implemented it.\n",
        "  * The instructors will review your code and may provide specific comments. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUleqQaRtAE4"
      },
      "source": [
        "### Preparing input data for binary classification\n",
        "\n",
        "The function below help you reshape the original data into a format that is easy to apply a binary classification algorithm. You are expected to specify a pair of digits as positive and negative labels. The function `prep_binary_classification()` returns a list containing a matrix `X` and a vector `y` as attributes. The matrix `X` has 784 (=28 x28) columns, and the number of rows equivalent to the number of positive + negative labels. The vector `y` is encoded as either 1 (for positive label) or 0 (for negative label) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abD-BiQh0pna"
      },
      "source": [
        "## DO NOT MODIFY THIS CELL\n",
        "## prepare a model given a pair of digits\n",
        "#' @param images - (r * c * n) array containing n images\n",
        "#' @param labels - Size n vector of containing n labels\n",
        "#' @param posLabel - The label(s) considered as positive label \n",
        "#' @param negLabel - The label(s) considered as negative label\n",
        "#' @return A list containing then following attributes\n",
        "#'         * X - Matrix of (nrow=number of selected labels, ncol=r*c)\n",
        "#'         * y - Vector labels, encoded as zeros (negative) and ones (positive)\n",
        "prep_binary_classification = function(images, labels, posLabel, negLabel) {\n",
        "  dims = dim(images)\n",
        "  X.pos = t(matrix(images[,,labels %in% posLabel],nrow=dims[1]*dims[2])) ## 784 * n1 matrix\n",
        "  X.neg = t(matrix(images[,,labels %in% negLabel],nrow=dims[1]*dims[2])) ## 784 * n1 matrix\n",
        "  return(list(X=rbind(X.pos, X.neg),y = c(rep(1,nrow(X.pos)),rep(0,nrow(X.neg)))))\n",
        "}"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmaKaLvx4baT",
        "outputId": "e049c9c4-86e8-41b6-8192-26c34a012575"
      },
      "source": [
        "## DO NOT MODIFY THIS CELL - IF YOU WANT TO CHANGE LABELS, USE TASK 1b.\n",
        "## Subsample training and test datasets\n",
        "trn = prep_binary_classification(train.X, train.Y, 1, 0)\n",
        "tst = prep_binary_classification(test.X, test.Y, 1, 0)\n",
        "## Check the dimensions of the data\n",
        "print(dim(trn$X))\n",
        "print(length(trn$y))\n",
        "print(dim(tst$X))\n",
        "print(length(tst$y))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] 12665   784\n",
            "[1] 12665\n",
            "[1] 2115  784\n",
            "[1] 2115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMF_GTK_C6Xy"
      },
      "source": [
        "#### ***TASK 1a : IMPLEMENT LOGISTIC RIDGE REGRESSION ON YOUR OWN*** (30 pts)\n",
        "\n",
        "Let $\\boldsymbol{x}_i$ be a row of `X`, and $\\sigma(z) = \\frac{1}{1+e^{-z}}$. Briefly, logistic Ridge regression will attempt to maximize the following penalized likelihood function.\n",
        "\n",
        "$$\n",
        "\\log L(\\boldsymbol{\\beta},\\lambda) = \\frac{1}{n}\\sum_{i=1}^n \\left[y_i\\log\\sigma\\left(\\boldsymbol{x}_i^T\\boldsymbol{\\beta}\\right) + (1-y_i)\\log\\left(1-\\sigma\\left(\\boldsymbol{x}_i^T\\boldsymbol{\\beta}\\right)\\right) \\right] - \\frac{\\lambda}{2n}\\|\\boldsymbol{\\beta}\\|_2^2\n",
        "$$\n",
        "\n",
        "which is equivalent to minimize $-\\log L(\\boldsymbol{\\beta},\\lambda)$. Some popular packages provide an easy way to perform this task. However, your task is to implement this algorithm yourself, utilizing numerical optimizations we learned from the class, without relying on external packages.\n",
        "\n",
        "In this exercise below, to save the computational time, we will <u>train the model using the ***test*** data</u> instead of the training data because test data is smaller in size. We will evaluate the model across all train data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of the ***Task 1a*** is to implement logistic regression on your own. Here is a basic guideline.\n",
        "* You need to implement `my.ridge.fit(X, y, lambda=1)` using only `base` and `stats` package.\n",
        "* The function should return a list, including at least the following two attributes:\n",
        "  * `par` : A vector of coefficients from logistic ridge regression with size `ncol(X)`\n",
        "  * `counts` : A length 2 vector containing the number of evaluations of the objective functions and gradients.\n",
        "  * Note that these attributes can be easily obtained from the output of `stats::optim()` function.  \n",
        "* Using `stats::optim()` function with `L-BFGS-B` algorithm is highly recommended.\n",
        "* Use $-\\log L(\\boldsymbol{\\beta},\\lambda)$ as the objective function and provide the gradient as needed.\n",
        "* For this required task, you are not allowed to use external libraries beyond default R packages (e.g. `base`, `utils`, `stats`)."
      ],
      "metadata": {
        "id": "cQ7sHdb0tULO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd0WGydp9G_X"
      },
      "source": [
        "#### <u>Task 1a - PART I - DESCRIPTION OF YOUR IMPLEMENTATION (WRITE ON YOUR OWN)</u>\n",
        "\n",
        "According to the penalized likelihood function above, we have to minimize $-logL(\\beta,\\lambda)$ and use the gradient descent method to find the optimal $\\beta$. The derivation are as follows.\n",
        "$$\\sigma(z) = \\frac{1}{1-e^{-z}} \\Rightarrow \\frac{d\\sigma}{dz} = \\sigma(z)(1-\\sigma(z))$$\n",
        "$$L = -logL(\\beta,\\lambda)=-\\frac{1}{n}\\sum_{i=1}^n[y_ilog\\sigma(x_i^T\\beta)+(1-y_i)log(1-\\sigma(x_i^T\\beta))]+\\frac{\\lambda}{2n}\\lVert\\beta\\rVert_2^2$$\n",
        "For each element $\\beta_j$ in the vector $\\beta$,\n",
        "\\begin{aligned}\n",
        "\\frac{dL}{d\\beta_j} &= -\\frac{1}{n}\\sum_{i=1}^n[\\frac{y_i}{\\sigma(x_i^T\\beta)}-\\frac{1-y_i}{1-\\sigma(x_i^T\\beta)}]\\frac{d\\sigma(x_i^T\\beta)}{d\\beta_j}+\\frac{\\lambda\\beta_j}{n}\\\\\n",
        "&= -\\frac{1}{n}\\sum_{i=1}^n[\\frac{y_i}{\\sigma(x_i^T\\beta)}-\\frac{1-y_i}{1-\\sigma(x_i^T\\beta)}]\\sigma(x_i^T\\beta)(1-\\sigma(x_i^T\\beta))x_{ij}+\\frac{\\lambda\\beta_j}{n}\\\\\n",
        "&= -\\frac{1}{n}\\sum_{i=1}^n[y_i(1-\\sigma(x_i^T\\beta))-(1-y_i)\\sigma(x_i^T\\beta)]x_{ij}+\\frac{\\lambda\\beta_j}{n}\\\\\n",
        "&= -\\frac{1}{n}\\sum_{i=1}^n[y_i-\\sigma(x_i^T\\beta)]x_{ij}+\\frac{\\lambda\\beta_j}{n}\\\\\n",
        "\\end{aligned}\n",
        "Thus, the gradient function is, $$\\frac{1}{n}\\sum_{i=1}^n[\\sigma(x_i^T\\beta)-y_i]x_{ij}+\\frac{\\lambda\\beta_j}{n}$$\n",
        "In this question, since we have to use `optim` function, we have to implement the loss function $L$ and the gradient function above as the input parameters of `optim` function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRbQgxWe-e_K"
      },
      "source": [
        "#### <u>Task 1a - PART II - ACTUAL IMPLEMENTATION </u>\n",
        "\n",
        "_Fill in the code in the cell below. You may add additional cells if needed._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxo6OwxL5W4X"
      },
      "source": [
        "# sigmoid function\n",
        "sigmoid = function(z){1/(1+exp(-z))}\n",
        "# define a clip function to make sure loss is finite value\n",
        "clip = function(x,min,max){\n",
        "  return(pmin(pmax(min,pmax(x,pmin(x,min))),max))\n",
        "}\n",
        "# loss function L\n",
        "Loss = function(X,y,beta,lambda){\n",
        "  count1 <<- count1+1 # record the number of evaluations\n",
        "  sigma = sigmoid(X%*%beta) # calculate the sigmoid function of each xi*beta\n",
        "  sigma = clip(sigma,10^(-15),1-10^(-15)) # make sure that 10^(-15) <= sigma <= 1-10^(-15)\n",
        "  n = length(y)\n",
        "  L = (t(-y)%*%log(sigma)-t(1-y)%*%log(1-sigma))/n + lambda/(2*n)*t(beta)%*%beta # calculate the loss function\n",
        "  return(L)\n",
        "}\n",
        "# gradient descent method\n",
        "gradient = function(X,y,beta,lambda){\n",
        "  count2 <<- count2+1 # record the number of evaluations\n",
        "  sigma = sigmoid(X%*%beta) # calculate the sigmoid function of each xi*beta\n",
        "  n = length(y)\n",
        "  grad = (t(X)%*%(sigma-y))/n + (lambda/n)*beta # calculate the gradient function\n",
        "  return(grad)\n",
        "}\n",
        "\n",
        "## TASK 1a: Implement my.ridge.fit() function that performs logistic regression\n",
        "#' @param X - (n x p) matrix of predictor variables. Assume that intercept term is already included in the input.\n",
        "#' @param y - A size n vector of zeros (negative) and ones (positives) as labels.\n",
        "#' @return A list containing at least the following two attributes, which can be passed from stats::optim()\n",
        "#'    * par - A size p vector of coefficients from logistic ridge regression.\n",
        "#'    * counts - The number of function calls made on objective function and gradients.\n",
        "#'    * You may include other attributes as you want.  \n",
        "my.ridge.fit = function(X, y, lambda=1) {\n",
        "  ## IMPLEMENT YOUR OWN FUNCTION HERE:\n",
        "  ## * You may define other functions inside, or outside this function\n",
        "  ## * Only base and stats packages are allowed. \n",
        "  ## * Using stats::optim() is highly recommended\n",
        "  ## * Your objective function should be -log L(\\beta,\\lambda) in the equation above.\n",
        "  ## * Note that you may have to deal with numerical precision challenges.\n",
        "  count1 <<- 0\n",
        "  count2 <<- 0\n",
        "  m = ncol(X)\n",
        "  # initialize beta as all 0's\n",
        "  beta <- matrix(rep(0,m), nrow = m)\n",
        "  # use 'optim' with L-BFGS-B algorithm\n",
        "  res = optim(beta, fn = Loss, gr = gradient, method = \"L-BFGS-B\", X = X, y = y, lambda = lambda)\n",
        "  # return coefficients and evaluation numbers\n",
        "  return(list(par = res$par, counts = c(count1,count2))) ## Make sure to return a list\n",
        "}"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKcIpeel85NT"
      },
      "source": [
        "### Evaluation of TASK 1a\n",
        "\n",
        "***DO NOT MODIFY the code fragment below.*** \n",
        "\n",
        "This code evaluates your output from your previous function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhtDn64yHWel",
        "outputId": "325580de-d057-4d83-8c4b-f30920c155c3"
      },
      "source": [
        "## DO NOT MODIFY THIS CELL\n",
        "tic <- proc.time()\n",
        "rst <- my.ridge.fit(tst$X, tst$y, 1) ## run the new method\n",
        "toc <- proc.time()\n",
        "cat(paste0(\"New ridge regression train took \",sprintf(\"%.5f\",(toc-tic)[3]),\" seconds\\n\")) \n",
        "cat(paste0(\"Number of (function, gradient) evaluations: (\",rst$counts[1],\",\",rst$counts[2],\")\\n\")) \n",
        "## Example output is given below."
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New ridge regression train took 3.18700 seconds\n",
            "Number of (function, gradient) evaluations: (81,81)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHIT3V68Ujmx",
        "outputId": "f91a0f1c-0c9a-448e-f265-6d5db056518f"
      },
      "source": [
        "## DO NOT MODIFY THIS CELL\n",
        "## Evaluate the prediction accuracy using train data (as test data was used for training)\n",
        "my.pred <- trn$X %*% rst$par \n",
        "## Tabulate the TP/FP/TN/FN\n",
        "print(data.frame(pred=as.integer(my.pred > 0), true=trn$y) %>% count(true,pred) %>% mutate(freq=n/sum(n)))\n",
        "## Example output is given below.\n",
        "cat(paste0(\"Accuracy = \", sum(as.integer(my.pred > 0) == trn$y)/length(my.pred)), \"\\n\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  true pred    n         freq\n",
            "1    0    0 5908 0.4664824319\n",
            "2    0    1   15 0.0011843664\n",
            "3    1    0    9 0.0007106198\n",
            "4    1    1 6733 0.5316225819\n",
            "Accuracy = 0.998105013817608 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_NOTE: The above output is merely an example, and it is okay not to have the same output. If your accuracy is much lower than above, you may need to improve it._"
      ],
      "metadata": {
        "id": "e2G4E2iA5YIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1a : Multi-class Classification of MNIST data\n",
        "\n",
        "The multi-class classfication of MNIST data is outlined below. \n",
        "\n",
        "1. Subset images/labels that contains only two or more labels labels (you may select a group digits).\n",
        "2. You are required to implement multi-class logistic Ridge regression by using the L-BFGS-B algorithm implemented in `optim()` function.\n",
        "  * You need to comment your code well and write a brief documentation to explain how you implemented it.\n",
        "  * The instructors will review your code and may provide specific comments. \n",
        "\n",
        "### Preparing input data for multi-class classification\n",
        "\n",
        "The function below help you reshape the original data into a format that is easy to apply a \n",
        "multi-class classification algorithm. You are expected to specify a pair of digits as positive and negative labels. \n",
        "\n",
        "The function `prep_multi_classification()` returns a list containing matrices `X` and `Y` as attributes. \n",
        "\n",
        "The matrix `X` has 784 (=28 x28) columns, and the number of rows equivalent to the number of positive + negative labels, similar to the output from `prep_binary_classification()`. \n",
        "\n",
        "The matrix `Y` is a one-hot encoded matrix (i.e. `rowSum(Y) == 1`) to have value of `1` when the multi-class label matches with the column index."
      ],
      "metadata": {
        "id": "P1Y77kF6r7oN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## DO NOT MODIFY THIS CELL\n",
        "## prepare data given a set of digits\n",
        "#' @param images - (r * c * n) array containing n images\n",
        "#' @param labels - Size n vector of containing n labels\n",
        "#' @param selectedLabels - A vector of k labels considered for multi-class clustering  \n",
        "#' @return A list containing then following attributes\n",
        "#'         * X - matrix of (nrow=number of selected labels, ncol=r*c)\n",
        "#'         * Y - (n * k) matrix of selected labels in one-hot encoding\n",
        "prep_multi_classification = function(images, labels, selectedLabels) {\n",
        "  dims = dim(images)\n",
        "  idx = which(labels %in% selectedLabels)\n",
        "\n",
        "  ## select images matching with selected labels and convert to matrix\n",
        "  X = t(matrix(images[,,idx],nrow=dims[1]*dims[2])) ## 784 * n matrix\n",
        "  n = nrow(X)   \n",
        "  k = length(selectedLabels) \n",
        "\n",
        "  ## create one-hot encoded matrix Y\n",
        "  matched_labels = match(labels[idx], selectedLabels)\n",
        "  Y = matrix(0, n, k)\n",
        "  Y[(matched_labels-1)*n + 1:n] <- 1    ## generate one-hot encoding matrix\n",
        "  return(list(X=X, Y=Y))\n",
        "}"
      ],
      "metadata": {
        "id": "HiEbSn6ht27V"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***TASK 1b : IMPLEMENT MULTI-CLASS CLASSIFICATION ON MNIST DATA*** (30 pts)\n",
        "\n",
        "Define  \n",
        "\n",
        "$$\\boldsymbol{\\tau}(\\boldsymbol{z}) = \\frac{e^{z}}{\\sum_{i=1}^k e^{z_i}}$$\n",
        "\n",
        "where $\\boldsymbol{\\tau}(\\boldsymbol{z}) \\in \\mathbb{R}^k$ and $k$ is the number of labels for multi-class classification. \n",
        "\n",
        "Then we can define the penalized likelihood function of multiclass logistic regression in terms of $B \\in \\mathbb{R}^{p \\times k}$.\n",
        "\n",
        "$$L(B,\\lambda) = \n",
        "\\frac{1}{n}\\sum_{i=1}^n \\boldsymbol{y}_i^T\\log \\boldsymbol{\\tau}\\left(\\boldsymbol{x}_i^T B\\right) - \\frac{\\lambda}{2n}\\|B\\|_2^2\n",
        "$$\n",
        "\n",
        "where $\\boldsymbol{x}_i \\in \\mathbb{R}^p$ is predictor variable and $\\boldsymbol{y}_i \\in \\mathbb{R}^k$ is one-hot encoding of multi-class labels for $i$-th observation. "
      ],
      "metadata": {
        "id": "M152_EiQuJw1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of the ***Task 1b*** is to implement multi-class logistic regression on your own. Here is a basic guideline.\n",
        "* You need to implement `my.multi.fit(X, Y, lambda=1)` using only `base` and `stats` package.\n",
        "* The function should return a list, including at least the following two attributes:\n",
        "  * `par` : A `(p x k)` matrix of coefficients ($B$ in the equation above) from logistic ridge regression.\n",
        "  * `counts` : A length 2 vector containing the number of evaluations of the objective functions and gradients.\n",
        "  * Note that these attributes can be easily obtained from the output of `stats::optim()` function.  \n",
        "* Using `stats::optim()` function with `L-BFGS-B` algorithm is highly recommended.\n",
        "* Use $-\\log L(\\boldsymbol{B},\\lambda)$ as the objective function and provide the gradient as needed.\n",
        "* For this required task, you are not allowed to use external libraries beyond default R packages (e.g. `base`, `utils`, `stats`).\n",
        "\n",
        "*Hint*: Because `optim()` function takes vector as argument, you need to pass $B$ as a vector, not as a matrix when using in objective and gradient functions, so some tweaking from the equation above may be necessary.  "
      ],
      "metadata": {
        "id": "K7MkZrrwuNKS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <u>Task 1b - PART I - DESCRIPTION OF YOUR IMPLEMENTATION (WRITE ON YOUR OWN)</u>\n",
        "\n",
        "For $B\\in\\mathbb{R}^{p\\times k}$, we can derive it into $k$ vectors as $(\\beta_1,\\beta_2,\\cdots,\\beta_k)$. Then we can derive the loss function $-L(B,\\lambda)$ as:\n",
        "\\begin{aligned}\n",
        "L &= -\\frac{1}{n}\\sum_{i=1}^ny_i^Tlog(\\frac{e^{x_i^T(\\beta_1,\\beta_2,\\cdots,\\beta_k)}}{\\sum_{j=1}^ke^{x_i^T\\beta_j}})+\\frac{\\lambda}{2n}\\lVert B\\rVert_2^2\\\\\n",
        "&= -\\frac{1}{n}\\sum_{i=1}^n\\sum_{j=1}^ky_{ij}log(\\frac{e^{x_i^T\\beta_j}}{\\sum_{j=1}^ke^{x_i^T\\beta_j}})+\\frac{\\lambda}{2n}\\lVert B\\rVert_2^2\\\\\n",
        "&= -\\frac{1}{n}\\sum_{i=1}^n\\sum_{j=1}^ky_{ij}[log(e^{x_i^T\\beta_j})-log(\\sum_{j=1}^ke^{x_i^T\\beta_j})]+\\frac{\\lambda}{2n}\\lVert B\\rVert_2^2\\\\\n",
        "&= -\\frac{1}{n}\\sum_{i=1}^n\\sum_{j=1}^ky_{ij}[x_i^T\\beta_j-log(\\sum_{j=1}^ke^{x_i^T\\beta_j})]+\\frac{\\lambda}{2n}\\lVert B\\rVert_2^2\\\\\n",
        "\\end{aligned}\n",
        "Taking gradient with respect to $\\beta_j$:\n",
        "\\begin{aligned}\n",
        "\\frac{dL}{d\\beta_j} &=  -\\frac{1}{n}\\sum_{i=1}^n\\sum_{j=1}^ky_{ij}(x_i^T - \\frac{x_i^Te^{x_i^T\\beta_j}}{\\sum_{j=1}^ke^{x_i^T\\beta_j}})+\\frac{\\lambda\\beta_j}{n}\\\\\n",
        "\\end{aligned}"
      ],
      "metadata": {
        "id": "y9z49FaS4k4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <u>Task 1b - PART II - ACTUAL IMPLEMENTATION </u>\n",
        "\n",
        "_Fill in the code in the cell below. You may add additional cells if needed._"
      ],
      "metadata": {
        "id": "6UWgJsW55ta2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################################\n",
        "## PLEASE MODIFY THIS CELL (or Add more cells if needed)\n",
        "##########################################################\n",
        "## TASK 1a: Implement my.multi.fit() function that performs logistic regression\n",
        "#' @param X - (n x p) matrix of predictor variables. Assume that intercept term is already included in the input.\n",
        "#' @param Y - (n x k) matrix of one-hot encoding of multi-class labels\n",
        "#' @return A list containing at least the following two attributes, which can be passed from stats::optim()\n",
        "#'    * par - A size p vector of coefficients from logistic ridge regression.\n",
        "#'    * counts - The number of function calls made on objective function and gradients.\n",
        "#'    * You may include other attributes as you want.  \n",
        "my.multi.fit = function(X, Y, lambda=1) {\n",
        "  ## IMPLEMENT YOUR OWN FUNCTION HERE:\n",
        "  ## * You may define other functions inside, or outside this function\n",
        "  ## * Only base and stats packages are allowed. \n",
        "  ## * Using stats::optim() is highly recommended\n",
        "  ## * Your objective function should be -log L(\\beta,\\lambda) in the equation above.\n",
        "  ## * Note that you may have to deal with numerical precision challenges.\n",
        "  ## * Note that you need to vectorize the matrix parameter B in the equation above.\n",
        "\n",
        "  return(list(par=..., counts=..., ...)) ## Make sure to return a list\n",
        "}"
      ],
      "metadata": {
        "id": "9paI186ZuXLO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 1b - Evaluation\n",
        "\n",
        "***WARNING: DO NOT MODIFY the code segments below.*** \n",
        "\n",
        "This code evaluates your output from your previous function. "
      ],
      "metadata": {
        "id": "3Wy10KLAucBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## DO NOT MODIFY THIS CELL - IF YOU WANT TO CHANGE LABELS, USE TASK 1b.\n",
        "## Subsample training and test datasets\n",
        "#trn = prep_multi_classification(train.X, train.Y, c(0,1,2))\n",
        "#tst = prep_multi_classification(test.X, test.Y, c(0,1,2))\n",
        "selected.labels = c(0, 1, 7)\n",
        "trn = prep_multi_classification(train.X, train.Y, selected.labels)\n",
        "tst = prep_multi_classification(test.X, test.Y, selected.labels)\n",
        "## Check the dimensions of the data\n",
        "print(dim(trn$X))\n",
        "print(dim(trn$Y))\n",
        "print(dim(tst$X))\n",
        "print(dim(tst$Y))"
      ],
      "metadata": {
        "id": "Z4PVyCN4QSmA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e5b6b28-93fd-4307-f309-dce9e30ede51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] 18930   784\n",
            "[1] 18930     3\n",
            "[1] 3143  784\n",
            "[1] 3143    3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## DO NOT MODIFY THIS CELL\n",
        "tic <- proc.time()\n",
        "rst <- my.multi.fit(tst$X, tst$Y, 1) ## run the new method\n",
        "toc <- proc.time()\n",
        "cat(paste0(\"New ridge regression train took \",sprintf(\"%.5f\",(toc-tic)[3]),\" seconds\\n\")) \n",
        "cat(paste0(\"Number of (function, gradient) evaluations: (\",rst$counts[1],\",\",rst$counts[2],\")\\n\")) \n",
        "## Example output is given below."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGNfhMMoQGlF",
        "outputId": "6c1fdd73-8b4c-40b6-f744-54ed40b5bb6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New ridge regression train took 1.86600 seconds\n",
            "Number of (function, gradient) evaluations: (34,34)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my.pred <- selected.labels[apply(trn$X %*% rst$par, 1, which.max)]\n",
        "true.pred <- selected.labels[apply(trn$Y, 1, which.max)]\n",
        "print(table(my.pred))\n",
        "## Tabulate the TP/FP/TN/FN\n",
        "print(data.frame(pred=my.pred, true=true.pred) %>% count(true,pred) %>% mutate(freq=n/sum(n)))\n",
        "## Example output is given below.\n",
        "\n",
        "cat(paste0(\"Accuracy = \", sum(my.pred==true.pred)/length(my.pred)), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BlaIJt0uoKw",
        "outputId": "36194f5e-38ee-4622-d552-7257a2454ad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my.pred\n",
            "   0    1    7 \n",
            "6676 6827 5427 \n",
            "  true pred    n         freq\n",
            "1    0    0 5907 0.3120443740\n",
            "2    0    1    5 0.0002641310\n",
            "3    0    7   11 0.0005810882\n",
            "4    1    0   78 0.0041204437\n",
            "5    1    1 6616 0.3494981511\n",
            "6    1    7   48 0.0025356577\n",
            "7    7    0  691 0.0365029054\n",
            "8    7    1  206 0.0108821976\n",
            "9    7    7 5368 0.2835710512\n",
            "Accuracy = 0.945113576333862 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BF8EEGpGV-G"
      },
      "source": [
        "#### <u>Task 1 - PART III - INTERPRET EVALUATION RESULTS (WRITE ON YOUR OWN)</u>\n",
        "\n",
        "***Please delete the text in this cell and fill in with your own.***\n",
        "\n",
        "_Briefly describe the followings: (it does not have to be long. 1-2 sentences each is fine.)_\n",
        "* _Does your result make sense to you? What was most surprising to you?_\n",
        "* _What was the challenging part in your implementation? How did you address the challenges?_\n",
        "* _How could your implementation be potentially improved?_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIsxpVAzL1d-"
      },
      "source": [
        "## Task 2 : Clustering of MNIST data with Expectation-Maximization (E-M) algorithm\n",
        "\n",
        "The second task is to implement an algorithm that performs multi-class clustering of MNIST images using the Expectation-Maximization algorithm.\n",
        "\n",
        "1. Subset images/labels that contains 3 or more labels (to make the problem simple, but not too simple).\n",
        "2. You are required to implement an E-M algorithm based on multivariate binomial distribution.\n",
        "  * You need to comment your code well and write a brief documentation to explain how you implemented it.\n",
        "  * The instructors will review your code and may provide specific comments. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0DbzD5L4eTM"
      },
      "source": [
        "### Pre-task: Prepare the data for clustering\n",
        "\n",
        "This section select a subset of labels from the test data (we work on test data rather than training to reduce response time), and cluster the corresponding images without using the labels at all. In this example, three labels - 0, 1, 7 are selected. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUb6mgwS0wCQ"
      },
      "source": [
        "## DO NOT MODIFY THIS CELL\n",
        "## prepare data given a set of digits\n",
        "#' @param images - (r * c * n) array containing n images\n",
        "#' @param labels - Size n vector of containing n labels\n",
        "#' @param selectedLabels - The label(s) considered for clustering  \n",
        "#' @return A list containing then following attributes\n",
        "#'         * X - Matrix of (nrow=number of selected labels, ncol=r*c)\n",
        "#'         * y - Vector selected labels\n",
        "prep_multiway_clustering = function(images, labels, selectedLabels) {\n",
        "  dims = dim(images)\n",
        "  idx = which(labels %in% selectedLabels)\n",
        "  X = t(matrix(images[,,idx],nrow=dims[1]*dims[2])) ## 784 * n matrix\n",
        "  return(list(X=X,y = labels[idx]))\n",
        "}"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcRhU3wY0x0H"
      },
      "source": [
        "## DO NOT MODIFY THIS CELL\n",
        "## Select 3 labels for clustering\n",
        "selected.labels = c(0, 1, 7)\n",
        "tst = prep_multiway_clustering(test.X, test.Y, selected.labels)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5LiSZ5e5kkS"
      },
      "source": [
        "### ***TASK 2: IMPLEMENT EM CLUSTERING ON YOUR OWN*** (40 pts)\n",
        "\n",
        "The goal of the first project is to implement 3-class clustering. Here is a basic guideline.\n",
        "* You need to implement `my.EM.clustering(X, nclust, max.iter, tol)` using only `base` and `stats` package.\n",
        "* The function should return a list, including at least the following two attributes:\n",
        "  * `mu` : A `ncol(X) * nclust` matrix of mean pixel strength (0-1 scale)  for each cluster. \n",
        "  * `est` : The best guess cluster of each image to one of the labels (i.e. in `1:nclust`). Note that the cluster id and the images labels do not have direct correspondence and may be permuted in different runs.\n",
        "  \n",
        "Here are useful guidelines to implement an E-M algorithm:\n",
        "* Consider the image as if it is a binary data. For example, consider all pixels with value greater than 127 as 1, otherwise as zero. This allows us to model the image as 784(=28x28)-dimensional binary vectors.\n",
        "* For convenience, ignore correlations between the dimensions, and model the conditional likelihood of data using independent Bernoulli random variables. For example, \n",
        "$$\n",
        "\\Pr(\\boldsymbol{x}_i | \\boldsymbol{\\mu}) = \\prod_{j=1}^p \\mu_{j}^{x}(1-\\mu_{j})^{1-x_i} \n",
        "$$\n",
        "where $\\boldsymbol{x}_i \\in \\{0,1\\}^p$ is binarized input image, and $\\boldsymbol{\\mu} \\in (0,1)^p$ is the parameters for a cluster, and $p$ is the dimension of input vector (e.g. 784). \n",
        "* The marginal likelihood of the data can be represented as a mixture of multiple components (clusters).\n",
        "$$\n",
        "\\Pr(\\boldsymbol{x}_i) = \\sum_{k} \\pi_k \\Pr(\\boldsymbol{x}_i | \\boldsymbol{\\mu}_k) \n",
        "$$\n",
        "where $\\pi_k$ represents mixing proportion of each component.\n",
        "* Then, you may want to implement an E-M algorithm to infer $\\boldsymbol{\\mu}_1, \\ldots, \\boldsymbol{\\mu}_K$ and other relevant parameters given observed data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBbsXdBy8OQb"
      },
      "source": [
        "#### <u>Task 2 - PART I - DESCRIPTION OF YOUR IMPLEMENTATION (WRITE ON YOUR OWN)</u>\n",
        "\n",
        "First, consider all pixels with values greater than 127 as 1, otherwise as 0, and this can cahnge an image into a $p$-dimensional binary vectors(in this problem, $p=784$). We consider a single multivariate random variable $x = (x_1,x_2, \\cdots, x_p)$, and $\\mu=(\\mu_1,\\mu_2,\\cdots,\\mu_p)$, which represent the input image binary vector and the univariate bernounlli distributed parameters. And then we can model the conditional likelihood,\n",
        "$$Pr(x|\\mu)=\\prod_{j=1}^p \\mu_j^{x_j}(1-\\mu_j)^{1-x_j}$$\n",
        "Since we have to model multiple clusters, suppose we have a mixture of $K$ Bernoulli distributions, and each cluster has a set of parameters $\\mu^{(k)}=(\\mu_1^{(k)},\\mu_2^{(k)},\\cdots,\\mu_p^{(k)})$, and a mixing proportion $\\pi_k$. And then the log-likelihood could be derived as:\n",
        "$$log(Pr(x|\\mu,\\pi))=log(\\sum_{k=1}^KI(x\\text{ is drawn form cluster }k)Pr(x|\\mu^{(k)}))=log(\\sum_{k=1}^K\\pi_kPr(x|\\mu^{(k)}))$$\n",
        "The likelihood above is for just a data point(or one image only), and now we have $N$ images inputs $X=(x^{(1)},\\cdots,x^{(N)})$, thus the log-likelihood function is,\n",
        "$$log(Pr(X|\\mu,\\pi))=log(\\prod_{n=1}^NPr(x^{(n)}|\\mu,\\pi))=\\sum_{n=1}^Nlog(\\sum_{k=1}^K\\pi_kPr(x^{(n)}|\\mu^{(k)}))$$\n",
        "Now, in order to maximize the log likelihood function, we use E-M algorithm and the first thing is to introduce the latent variables $Z = (z^{(1)},\\cdots,z^{(N)})$, and $z^{(n)}=(z_1^{(n)},\\cdots,z_K^{(n)})$ for $K$ different clusters,\n",
        "$$Pr(z^{(n)}|\\pi)=\\prod_{k=1}^K\\pi_k^{z_k^{(n)}},Pr(x^{(n)}|z^{(n)},\\mu,\\pi)=\\prod_{k=1}^K(Pr(x^{(n)}|\\mu^{(k)}))^{z_k^{(n)}}$$\n",
        "Thus, the conditional log-likelihood funtion of $X$ and $Z$ could be derived as,\n",
        "\\begin{aligned}\n",
        "log[Pr(X,Z|\\mu,\\pi)] &= log\\left[\\prod_{n=1}^NPr(x^{(n)}|z^{(n)},\\mu,\\pi)Pr(z^{(n)}|\\pi)\\right]\\\\\n",
        "&=log\\left\\{\\prod_{n=1}^N\\left[\\prod_{k=1}^K(Pr(x^{(n)}|\\mu^{(k)}))^{z_k^{(n)}}\\right]\\left[\\prod_{k=1}^K\\pi_k^{z_k^{(n)}}\\right]\\right\\}\\\\\n",
        "&=\\sum_{n=1}^N\\left[\\sum_{k=1}^Kz_k^{(n)}log[Pr(x^{(n)}|\\mu^{(k)})]+\\sum_{k=1}^Kz_k^{(n)}log\\pi_k\\right]\\\\\n",
        "&=\\sum_{n=1}^N\\sum_{k=1}^Kz_k^{(n)}\\left[log[Pr(x^{(n)}|\\mu^{(k)})]+log\\pi_k\\right]\\\\\n",
        "&=\\sum_{n=1}^N\\sum_{k=1}^Kz_k^{(n)}\\left[log\\left[\\prod_{j=1}^p(\\mu_j^{(k)})^{x_j^{(k)}}(1-\\mu_j^{(k)})^{(1-x_j^{(k)})}\\right]+log\\pi_k\\right]\\\\\n",
        "&=\\sum_{n=1}^N\\sum_{k=1}^Kz_k^{(n)}\\left[\\sum_{j=1}^p\\left[{x_j^{(k)}}log(\\mu_j^{(k)})+{(1-x_j^{(k)})}log(1-\\mu_j^{(k)})\\right]+log\\pi_k\\right]\n",
        "\\end{aligned}\n",
        "Also, we can have the expectation of latent variables,\n",
        "$$\\eta(z_k^{(n)})=E\\left[z_k^{(n)}|x^{(n)}\\mu,\\pi\\right]=Pr\\left[z_k^{(n)}=1|x^{(n)},\\mu,\\pi\\right]=\\frac{\\pi_k\\prod_{j=1}^p(\\mu_j^{(k)})^{x_j^{(k)}}(1-\\mu_j^{(k)})^{(1-x_j^{(k)})}}{\\sum_{k=1}^K\\pi_k\\prod_{j=1}^p(\\mu_j^{(k)})^{x_j^{(k)}}(1-\\mu_j^{(k)})^{(1-x_j^{(k)})}}$$\n",
        "For the M-step, in order to maximize the log-likelihood function with respect to the posterior distribution of latent variable $\\eta(z_k^{(n)})$, set the derivative to 0 and solve the equation,\n",
        "$$\\frac{d}{d\\mu_j^{(k)}}E_Z\\left[log(Pr(X,Z|\\mu,\\pi))\\right]=\\sum_{n=1}^N\\eta(z_k^{(n)})\\left[\\frac{x_j^{(n)}}{\\mu_j^{(k)}}+\\frac{1-x_j^{(n)}}{1-\\mu_j^{(k)}}\\right]=0$$\n",
        "$$\\mu_j^{(k)}=\\frac{\\sum_{n=1}^N\\eta(z_k^{(n)})x_j^{(n)}}{\\sum_{n=1}^N\\eta(z_k^{(n)})}$$\n",
        "Meantime, we have also to find the $\\pi_k$ to maximize the log-likelihood function, with a contraint of $\\sum_{k=1}^K\\pi_k=1$ since it is the mixing proportion, keeping the $\\mu_j^{(k)}$ as the result above:\n",
        "$$\\frac{d}{d\\pi_k}\\left[-\\sum_{n=1}^N\\sum_{k=1}^K\\eta(z_k^{(n)})log\\pi_k+\\lambda(\\sum_{k=1}^{K}\\pi_k-1)\\right]=0$$\n",
        "$$-\\sum_{n=1}^N\\frac{\\eta(z_k^{(n)})}{\\pi_k}+\\lambda=0\\Rightarrow\\pi_k=\\frac{\\sum_{i=1}^N\\eta(z_k^{(n)})}{\\lambda}$$\n",
        "Now we are going to solve the value of $\\lambda$, with the same method, setting the derivative to 0,\n",
        "$$\\frac{d}{d\\lambda}\\left[-\\sum_{n=1}^N\\sum_{k=1}^K\\eta(z_k^{(n)})(log[\\sum_{i=1}^N\\eta(z_k^{(n)})]-log\\lambda)+\\left(\\sum_{k=1}^K\\sum_{i=1}^N\\eta(z_k^{(n)})-\\lambda\\right)\\right]=0$$\n",
        "$$\\frac{1}{\\lambda}\\sum_{n=1}^N\\sum_{k=1}^K\\eta(z_k^{(n)})-1=0\\Rightarrow\\lambda=\\sum_{n=1}^N\\sum_{k=1}^K\\eta(z_k^{(n)})$$\n",
        "Thus,\n",
        "$$\\pi_k=\\frac{\\sum_{i=1}^N\\eta(z_k^{(n)})}{\\sum_{n=1}^N\\sum_{k=1}^K\\eta(z_k^{(n)})}$$\n",
        "For the implentation of this algorithm, we have to calculate $\\eta(z_k^{(n)})$ in the E-step, and update $\\mu$ and $\\pi$ in the M-step, and since we have to keep evey elements of $\\mu$ and $\\pi$ positive to avoid infinite value, we can add a tolerance (1e-100). In each iteration, we will do the E-step and M-step, and calculate the log-likelihood function, then compare the difference between 2 likelihood with the tolerance, if the difference is enough small, we can regard it as converged, and return the optimal solutions.\\\n",
        "For the initialization, $\\mu$ is randomly sampled from Uniform distribution(0,1), and normalized each column. $\\pi$ is equally selected as $\\frac{1}{K}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DngiJfgl89jl"
      },
      "source": [
        "#### <u>Task 2 - PART II - ACTUAL IMPLEMENTATION </u>\n",
        "\n",
        "_Fill in the code below. You may add additional cells if needed._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7pmyJbl00vm"
      },
      "source": [
        "##########################################################\n",
        "## PLEASE MODIFY THIS CELL (or Add more cells if needed)\n",
        "##########################################################\n",
        "## Implement your own clustering algorithm\n",
        "\n",
        "# E-step\n",
        "Estep = function(X,mu,p){ # X as input data, mu as parameters, p as pi(proportion)\n",
        "   log_eta <<- matrix(0, nrow = nrow(X),ncol = K)\n",
        "   log_eta <<- X%*%t(log(mu))+(1-X)%*%t(log(1-mu)) # log-eta values\n",
        "   for (i in 1:K){\n",
        "     log_eta[,i] = log_eta[,i] + log(p[i]) # for each column add a same pi value for log\n",
        "   }\n",
        "   max_log_eta <<- apply(log_eta, 1, max)\n",
        "   eta <<- exp(sweep(log_eta, 1, max_log_eta)) # each row minus the max value of the row\n",
        "   sum_eta <<- apply(eta, 1, sum)\n",
        "   eta <<- t(t(eta)/sum_eta) # normalize the eta matrix\n",
        "   logL <<- sum(max_log_eta+log(sum_eta)) # log-likelihood function\n",
        "}\n",
        "# M-step\n",
        "Mstep = function(X,eta){\n",
        "  sum_eta_col = apply(eta, 2, sum)\n",
        "  # update mu and pi, avoid infinite value\n",
        "  mu <<- (t(eta)%*%X)/sum_eta_col + 1e-100*ncol(eta)\n",
        "  p <<- sum_eta_col/sum(sum_eta_col) + 1e-100\n",
        "}\n",
        "# check convergence\n",
        "check.tol = function(fmax,fmin,ftol){\n",
        "  delta = abs(fmax - fmin)\n",
        "  accuracy = (abs(fmax) + abs(fmin))*ftol\n",
        "  return(delta < (accuracy + tol))\n",
        "}\n",
        "\n",
        "#' @param X : (n * 784) matrix of images (binarizing the matrix at X[i,j] > 127 is highly recommended)\n",
        "#' @param nclust : Number of clusters to generate\n",
        "#' @param max.iter : Maximum number of iteration, may end early based on convergence criteria\n",
        "#' @param tol : Parameters for determining convergence.\n",
        "#' @return A list containing at least the following attributes:\n",
        "#'    * mus - A [ ncol(X) * nclust ] matrix of mean pixel strength (0-1 scale)  for each cluster.\n",
        "#'    * est - The best guess cluster of each image to one of the labels (i.e. in `1:nclust`). \n",
        "my.EM.clustering = function(X, nclust, max.iter, tol) {\n",
        "  set.seed(680)\n",
        "  # convert input data to binary vectors\n",
        "  X[X<=127] <- 0\n",
        "  X[X>127] <- 1\n",
        "  K <<- nclust\n",
        "  max_iter = 1000L\n",
        "  tol <<- 1e-6\n",
        "  loglik_tol <<- 1e-6\n",
        "  loglik_list = NULL\n",
        "  # initialization\n",
        "  p <<- rep(1.0/K, length = K)\n",
        "  N = nrow(X)\n",
        "  M = ncol(X) # in the equation above M = p\n",
        "  mu <<- matrix(0, nrow = K, ncol = M)\n",
        "  for (i in 1:M){\n",
        "    mu[,i] <<- runif(K)\n",
        "    mu[,i] <<- mu[,i]/sum(mu[,i])\n",
        "  }\n",
        "  logL <<- -Inf\n",
        "  convergence = 0\n",
        "  # E-M algorithm\n",
        "  for (iter in 1:max_iter){\n",
        "    loglik0 <- logL # temperate variable to save last log-likelihood function\n",
        "    Estep(X, mu, p)\n",
        "    Mstep(X, eta)\n",
        "    loglik_list = c(loglik_list, logL) # save the log-likelihood function values\n",
        "    if (check.tol(loglik0, logL, loglik_tol)){\n",
        "      convergence = 0 # converged\n",
        "      break\n",
        "    }\n",
        "  }\n",
        "  mus = t(mu)\n",
        "  Estep(X, mu, p)\n",
        "  est <- apply(eta, 1, which.max)\n",
        "  ## Make sure to return at least two attributes (or more)\n",
        "  return(list(mus=mus,est=est,convergence=convergence,mu=mu,p=p,loglik_list=loglik_list,iter=iter))\n",
        "}"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhaGu1zt9Hig"
      },
      "source": [
        "### Evaluation of TASK 2\n",
        "\n",
        "***DO NOT MODIFY The code fragment below.***\n",
        "\n",
        "This code evaluates your output from your previous function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiF6wjGT1Wrw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7317debe-a49c-475b-cabb-8a55a76d0c43"
      },
      "source": [
        "## DO NOT MODIFY THIS CELL\n",
        "## Run the clustering algorithm from the selected Data\n",
        "tic()\n",
        "rst = my.EM.clustering(tst$X,length(selected.labels))\n",
        "toc()\n",
        "## NOTE: This step took 2.894 seconds from a working example with an E-M algorithm"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.082 sec elapsed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "id": "q3MXRhHS1Yej",
        "outputId": "9111a46d-ac7c-46c0-ce5b-eca879570568"
      },
      "source": [
        "## DO NOT MODIFY THIS CELL\n",
        "## Visualize rst$mus as images.\n",
        "options(repr.plot.width=4, repr.plot.height=1)\n",
        "par(mfrow=c(1,3))\n",
        "par(mar=c(0,0,0,0))\n",
        "apply(rst$mus,2,function(x) { image(matrix(x,28,28)[,28:1]*255,col=gray((0:255)/255)) })\n",
        "## below is a result from a working example with an E-M algorithm"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "NULL"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "plot without title"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAB4CAMAAAAdSNE9AAAC2VBMVEUAAAABAQECAgIDAwME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\nCLxd8H8DF/G4dCfM0AAAAABJRU5ErkJggg=="
          },
          "metadata": {
            "image/png": {
              "width": 240,
              "height": 60
            }
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEEnh-dH1dkY",
        "outputId": "efc710a6-8646-4549-cbd1-a76e888c58cb"
      },
      "source": [
        "## DO NOT MODIFY THIS CELL\n",
        "## Tabulate the contingency tables to understand the accuracy \n",
        "print(data.frame(est=rst$est, true=tst$y) %>% count(true,est) %>% mutate(freq=n/sum(n)))\n",
        "## below is a result from a working example with an E-M algorithm"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  true est    n         freq\n",
            "1    0   1   10 0.0031816736\n",
            "2    0   2  968 0.3079860006\n",
            "3    0   3    2 0.0006363347\n",
            "4    1   1   29 0.0092268533\n",
            "5    1   2    4 0.0012726694\n",
            "6    1   3 1102 0.3506204263\n",
            "7    7   1  943 0.3000318167\n",
            "8    7   2   33 0.0104995227\n",
            "9    7   3   52 0.0165447025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Note that the results above are based on a suggestive implementation, and you do not have to have exactly the same results (but comparable results are expected to obtain full credit)_"
      ],
      "metadata": {
        "id": "il1CcROP8Noi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1qNUBJk9CjX"
      },
      "source": [
        "#### <u>Task 2 - PART III - INTERPRET EVALUATION RESULTS (WRITE ON YOUR OWN)</u>\n",
        "\n",
        "* _Did your E-M algorithm always converge to the intended solution or not? Why or why not?_ \\\n",
        "Since the E-M algorithm never decrease the log-likelihood, thus it will always converge to an optimal solution. However, it converegs to a local optimal solution, so we can not make sure that it will converge to intended solution.\n",
        "* _Does your result make sense. What surprise you the most?_ \\\n",
        "My result makes sense obviously. The visualiztion of the result can clearly show  the shape of 3 different digits, and the accuracy of the result (7-1,0-2,1-3) reaches over 95%, which surprises me the most. Also, I find that the proportion of wrong classification  of differnent digits is not the same, and in my result, digit '7' has the highest probability to be wrongly classified.\n",
        "* _What was the tricky part in your implementation? How did you address the challenges?_ \\\n",
        "\n",
        "* _How could your implementation be potentially improved?_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikB5NMUs9pd8"
      },
      "source": [
        "### ***TASK 3: IMPROVE THE METHODS ABOVE*** (optional for extra credit)\n",
        "\n",
        "This is an open-ended task. You are encouraged to perform additional follow-ups on classification & clustering tasks above. Examples include the following:\n",
        "  * (Easy) Try different sets of labels to understand the data-dependent differences in the classification/clustering performance. Also, try to evaluate the robustness of your algorithm based on different initial points.\n",
        "  * (Medium) Identify issues with current implementations and try to find a way to improve the algorithm, in terms of accuracy or numerical stability.\n",
        "  * (Hard) Try to implement another algorithm and compare the performance between the algorithms. Describe which ones are better in which cases and justify your answer.\n",
        "\n",
        "Add additional code or text cells as needed below. Make sure to include your brief report on your work so that instructors can understand what you accomplished clearly.\n",
        "\n",
        "For this optional task, you are allowed to use any external libraries if needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qn6v2ifi9scT"
      },
      "source": [
        "##########################################################\n",
        "## PLEASE MODIFY THIS CELL (or Add more cells if needed)\n",
        "## To perform TASK 3 (optional)\n",
        "##########################################################"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}